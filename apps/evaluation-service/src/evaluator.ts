import type {
  EvaluationRequest,
  EvaluationResult,
  GoldenTestCase,
  RegressionReport,
} from '@ace/core-types';
import { createLogger } from '@ace/shared-logging';
import { evaluateAnswerAccuracy } from './evaluators/answer-accuracy';
import { evaluateEvidenceCoverage } from './evaluators/evidence-coverage';
import { evaluateFaithfulness } from './evaluators/faithfulness';

const logger = createLogger('evaluator');

/**
 * Comprehensive Evaluation Service
 * Runs all evaluation metrics and produces overall assessment
 */

export async function runEvaluation(request: EvaluationRequest): Promise<EvaluationResult> {
  const startTime = Date.now();
  const errors: string[] = [];
  const warnings: string[] = [];

  try {
    logger.info('Running evaluation', { testCaseId: request.testCaseId });

    // Run faithfulness evaluation (most critical)
    const faithfulness = await evaluateFaithfulness(
      request.generatedText,
      request.retrievedContext,
    );

    // Run evidence coverage
    const evidenceCoverage = await evaluateEvidenceCoverage(
      request.generatedText,
      request.retrievedContext,
      request.query,
    );

    // Run answer accuracy if expected output provided
    let answerAccuracy: Awaited<ReturnType<typeof evaluateAnswerAccuracy>> | undefined;
    if (request.expectedOutput) {
      answerAccuracy = await evaluateAnswerAccuracy(
        request.generatedText,
        request.expectedOutput,
        request.query,
      );
    }

    // Calculate overall score (weighted average)
    // Faithfulness is most important (60%), Evidence Coverage (30%), Answer Accuracy (10%)
    let overallScore = faithfulness.score * 0.6 + evidenceCoverage.score * 0.3;
    if (answerAccuracy) {
      overallScore =
        faithfulness.score * 0.5 + evidenceCoverage.score * 0.3 + answerAccuracy.score * 0.2;
    }

    // Determine pass/fail
    const faithfulnessThreshold = 0.97;
    const coverageThreshold = 0.8;
    const passed =
      faithfulness.score >= faithfulnessThreshold && evidenceCoverage.score >= coverageThreshold;

    // Add warnings for borderline scores
    if (faithfulness.score < faithfulnessThreshold) {
      errors.push(
        `Faithfulness score ${faithfulness.score.toFixed(3)} below threshold ${faithfulnessThreshold}`,
      );
    }
    if (faithfulness.score >= faithfulnessThreshold && faithfulness.score < 0.99) {
      warnings.push(
        `Faithfulness score ${faithfulness.score.toFixed(3)} is passing but close to threshold`,
      );
    }
    if (evidenceCoverage.score < coverageThreshold) {
      errors.push(
        `Evidence coverage ${evidenceCoverage.score.toFixed(3)} below threshold ${coverageThreshold}`,
      );
    }

    const result: EvaluationResult = {
      testCaseId: request.testCaseId || 'ad-hoc',
      passed,
      faithfulness,
      evidenceCoverage,
      answerAccuracy,
      overallScore,
      errors,
      warnings,
      evaluationTimeMs: Date.now() - startTime,
      timestamp: new Date().toISOString(),
    };

    logger.info('Evaluation complete', {
      testCaseId: result.testCaseId,
      passed,
      faithfulness: faithfulness.score,
      evidenceCoverage: evidenceCoverage.score,
      overallScore,
    });

    return result;
  } catch (error) {
    logger.error('Evaluation failed', { error });
    throw error;
  }
}

/**
 * Load golden dataset from file
 */
export async function loadGoldenDataset(version = 'v1'): Promise<GoldenTestCase[]> {
  try {
    const filePath = `${import.meta.dir}/../golden-dataset/${version}.json`;
    const file = Bun.file(filePath);
    const content = await file.json();
    return content.testCases as GoldenTestCase[];
  } catch (error) {
    logger.error('Failed to load golden dataset', { version, error });
    throw new Error(
      `Failed to load golden dataset: ${error instanceof Error ? error.message : 'Unknown error'}`,
    );
  }
}

/**
 * Run regression tests against golden dataset
 * This is the mandatory CI/CD gate (specs/architecture_blueprint.md Section 7.2)
 */
export async function runRegressionTests(version = 'v1'): Promise<RegressionReport> {
  const runId = `regression-${Date.now()}`;
  const startTime = Date.now();

  logger.info('Starting regression test run', { runId, version });

  try {
    const testCases = await loadGoldenDataset(version);
    const results: EvaluationResult[] = [];

    // Run tests sequentially to avoid overwhelming Ollama
    for (const testCase of testCases) {
      logger.info('Running test case', { id: testCase.id, category: testCase.category });

      try {
        const request: EvaluationRequest = {
          generatedText: '', // Will be generated by calling inference service
          retrievedContext: testCase.input.context || '',
          query: testCase.input.query,
          expectedOutput: testCase.expectedOutput.text,
          testCaseId: testCase.id,
        };

        // For now, use context as generated text (stub)
        // TODO: Call inference service to generate actual response
        request.generatedText = testCase.input.context || 'No context available';

        const result = await runEvaluation(request);
        results.push(result);
      } catch (error) {
        logger.error('Test case failed', { id: testCase.id, error });
        results.push({
          testCaseId: testCase.id,
          passed: false,
          faithfulness: {
            score: 0,
            claims: [],
            totalClaims: 0,
            groundedClaims: 0,
            ungroundedClaims: 0,
            evaluationTimeMs: 0,
          },
          evidenceCoverage: {
            score: 0,
            totalEvidencePoints: 0,
            coveredPoints: 0,
            missedPoints: [],
            reasoning: 'Test execution failed',
            evaluationTimeMs: 0,
          },
          overallScore: 0,
          errors: [
            `Test execution failed: ${error instanceof Error ? error.message : 'Unknown error'}`,
          ],
          warnings: [],
          evaluationTimeMs: 0,
          timestamp: new Date().toISOString(),
        });
      }
    }

    // Calculate summary statistics
    const totalTests = results.length;
    const passed = results.filter((r) => r.passed).length;
    const failed = totalTests - passed;

    const averageFaithfulness =
      results.reduce((sum, r) => sum + r.faithfulness.score, 0) / totalTests;
    const averageEvidenceCoverage =
      results.reduce((sum, r) => sum + r.evidenceCoverage.score, 0) / totalTests;
    const averageOverallScore = results.reduce((sum, r) => sum + r.overallScore, 0) / totalTests;

    // Determine recommendation
    const criticalFailures = results
      .filter((r) => !r.passed)
      .map((r) => `${r.testCaseId}: ${r.errors.join(', ')}`);

    let recommendation: 'PASS' | 'FAIL' | 'REVIEW_REQUIRED';
    if (failed === 0 && averageFaithfulness >= 0.97) {
      recommendation = 'PASS';
    } else if (averageFaithfulness < 0.95 || failed > totalTests * 0.2) {
      recommendation = 'FAIL';
    } else {
      recommendation = 'REVIEW_REQUIRED';
    }

    const report: RegressionReport = {
      runId,
      timestamp: new Date().toISOString(),
      totalTests,
      passed,
      failed,
      averageFaithfulness,
      averageEvidenceCoverage,
      averageOverallScore,
      results,
      summary: {
        criticalFailures,
        warnings: results.flatMap((r) => r.warnings),
        recommendation,
      },
      durationMs: Date.now() - startTime,
    };

    logger.info('Regression test run complete', {
      runId,
      passed,
      failed,
      recommendation,
      averageFaithfulness,
    });

    return report;
  } catch (error) {
    logger.error('Regression test run failed', { runId, error });
    throw error;
  }
}
